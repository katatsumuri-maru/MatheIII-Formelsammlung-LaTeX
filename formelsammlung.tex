\documentclass[6pt,a4paper,landscape]{article}
\pagestyle{empty}
\usepackage[landscape, left=0.5cm, right=0.5cm, top=0.5cm, bottom=0.5cm]{geometry}
\usepackage{paracol}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{xcolor}
\begin{document}
\footnotesize
\begin{paracol}{3}

\section{Funktionen in $\mathbb{R}^n$}
\subsection{Metrik}

Eine Funktion \( d: \mathbb{R}^n \times \mathbb{R}^n \to \mathbb{R}_0^+ \) mit \( \forall x, y, z \in \mathbb{R}^n \):

\noindent
\textbf{Positive Definitheit}

$
    d(x, y) \geq 0 \quad \text{und} \quad d(x, y) = 0 \iff x = y
$

\noindent
\textbf{Symmetrie}
$
    d(x, y) = d(y, x)
$

\noindent
\textbf{Dreiecksungleichung}
$
    d(x, y) \leq d(x, z) + d(z, y)
$

\noindent
\textbf{Triviale Metrik:}

$
d(x,y) = \begin{cases}
1 &\text{falls } x \neq y \\
0 &\text{falls } x = y \\
\end{cases}
$

\noindent
\textbf{Französische Eisenbahnmetrik:}

$
d(x,y) = \begin{cases}
||x-y|| &\text{falls } \exists \lambda \in \mathbb R : x = \lambda y \\
||x|| + ||y|| &\text{sonst} \\
\end{cases}
$

\subsection{Normen in $\mathbb R ^n$}

Eine Funktion $||.|| : \mathbb R^n \to \mathbb R^+_0$ mit

\noindent
\textbf{Positive Definitheit}
$
 ||x|| \ge 0 \land ||x|| = 0 \iff x = 0
$

\noindent
\textbf{Homogenität}
$
 ||\lambda x|| = |\lambda| ||x||
$

\noindent
\textbf{Dreiecksungleichung}
$
 ||x+y|| \le ||x|| + ||y||
$

\textbf{$l_1$-Norm}
$
||x||_1 = |x_1| + \dots + |x_n|
$

\textbf{$l_p$-Norm}
$
||x||_p = \sqrt[p]{x_1^p+\dots+x_n^p}
$

\textbf{Maximum-Norm}
$
||x||_\infty = \max\{ |x_i| : i=1, \dots, n\}
$

\noindent
\textbf{Satz}
Jede Norm induziert über $d(x,y) = ||x-y||$ eine Metrik.

\subsection{Bilinearform}
Eine Funktion $\langle.,. \rangle_A : \mathbb R^n \times \mathbb R^n \to \mathbb R$ heißt billinearform, wenn sie in beiden Argumenten linear ist:

$
\langle \alpha x + \beta y, z\rangle_A = \alpha \langle x,z \rangle_A + \beta \langle y,z \rangle_A
$

$
\langle z, \alpha x + \beta y \rangle_A = \alpha \langle z,x \rangle_A + \beta \langle z,y\rangle_A
$

\noindent
Weitere, mögliche Eigenschaften:

\textbf{Symmetrie}: $\forall x,y \in \mathbb R^n :\langle x,y \rangle_A = \langle y, x \rangle_A$

\textbf{Positive Definitheit}: $\forall x \in \mathbb R^n \land x \neq 0: \langle x,x \rangle_A > 0$

\noindent
Jede Quadratische Matrix $B$ erzeugt eine Bilinearform über $x^T B y$

\subsection{Topologie in $\mathbb R^n$}

\textbf{Offene Kugel mit Radius r um a:} 

$U_R(a) := \{x \in \mathbb R^n: ||x-a|| < r \}$

\noindent
\textbf{Innerer Punkt $a \in A \subseteq \mathbb R^n$:} 
$\exists \varepsilon > 0: U_\varepsilon(a) \subseteq A$

\noindent 
\textbf{Offenes Intervall:} Jeder Punkt ist innerer Punkt: 

$\forall a \in A : \exists \varepsilon > 0: U_\varepsilon(a) \subseteq A$
\indent bsp: $\mathbb R^n, \{\}, U_r(a)$

\noindent
\textbf{Abgeschlossenes Intervall:}
Komplement ist offen

bsp: $\mathbb R^n, \{ \}$

\noindent
\textbf{Beschränkt:}
$\exists M \in \mathbb R: A \subset U_M(0)$

\noindent
\textbf{Kompakt:} Abgeschlossen und beschränkt

\noindent
\textbf{Mengenoperationen}

$
(U \cup V)^C = U^C \cap V^C
$

$
(U \cap V)^C = U^C \cup V^C
$


\switchcolumn

\setcounter{section}{1}
\setcounter{subsection}{4}

\subsection{Folgen in $\mathbb{R}^n$}

\textbf{Konvergenz:}

$\exists g \in \mathbb R^n :\forall \varepsilon > 0: \exists M \in \mathbb N: \forall m \ge M: ||a_m -g|| < \varepsilon$

\noindent
\textbf{Mehrdimensionale Konvergenz:} Konvergiert genau dann, wenn alle Komponenten gegen den jeweiligen Wert konvergieren.

\noindent
\textbf{Cauchyfolge:}

$\forall \varepsilon > 0: \exists M \in \mathbb N; \forall s,m \ge M: ||a_s - a_m|| < \varepsilon$

\noindent
\textbf{Banachraum:}
Jede Cauchyfolge Konvergiert und die Metrik wird durch eine Norm induziert

Bsp: $R^n$ mit $l_p$-Norm

\subsection{Funktionen}
\textbf{Graph von $f: \mathbb R^n \to \mathbb R^m$:}

\noindent
$\Gamma_f := \{(x_1, \dots, x_n , z_1, \dots, z_m \in) D \times \mathbb R ^m: z_i = f_i(x_1, \dots, x_n)\}$

\noindent
\textbf{Höhen/Niveaulinie von $f: D \subseteq \mathbb R^2 \to \mathbb R$}

$H_c(f) := \{(x,y) \in D \mid f(x,y) = c\}$

\noindent \textbf{Niveaufläche von $f:D \subseteq \mathbb R^n \to \mathbb R$:}

$H_c(f) := \{(x_1,\dots, x_n) \in D \mid f(x_1, \dots, x_n) = c\}$

\noindent
\textbf{Eindimensionaler Schnitt}

$f(a_1, \dots, a_{i-1}, x_i, a_{i+1}, a_n)$ mit $a_k$ konst.

\noindent
\textbf{Kurve}

Ist $f : \mathbb R^n \to \mathbb R^m$ eine Funktion mit $f (x_1, \dots, x_n)$, dann ist $\phi : \mathbb R \to \mathbb R^n$ eine Kurve im $\mathbb R^n$ und $f (\phi(t))$ die Funktion entlang dieser Kurve.

\noindent
\textbf{Stetigkeit von Funktionen $\mathbb R^n \to \mathbb R^m$ ($\varepsilon-\delta$-Kriterium)}
Eine Funktion ist Stetig in einem Punkt a, gdw. Gilt:
$\forall \varepsilon > 0: \exists \delta >0: ||x-a|| < \delta \implies ||f(x) - f(a) || < \varepsilon$
\newline\noindent 
\textbf{Satz}
Eine Funktion $f : D \subseteq \mathbb R^n \to \mathbb R ^m$ ist genau dann stetig in $a \in D$, wenn dies
für die Funktionen $f_1,\dots, f_m$ gilt.

\noindent \textbf{Polynom in $n$ Variablen vom Grad $m$}

\noindent
\[
\sum_{i=0}^m \;\; \sum_{\substack{\alpha \in \mathbb{N}_0^n \\ |\alpha| = i}} a_\alpha \; x_1^{\alpha_1} \cdots x_n^{\alpha_n}
\]
mit $\alpha = (\alpha_1, \dots, \alpha_n)$ Multiindex, $a_\alpha \in \mathbb{R}$ (Vorfaktoren) und $|\alpha| = \alpha_1 + \dots + \alpha_n$.



\section{Differenzierbarkeit in $\mathbb R^n$}

\subsection{Ableitungsregeln in $\mathbb R^1$}

\noindent
\textbf{Linearität}
$ (f + g)' = f' + g' \qquad (c \cdot f)' = c \cdot f' $

\noindent
Produkt $ (f \cdot g)' = f' \cdot g + f \cdot g'\quad$Quot.$  \left( \frac{f}{g}\right)' = \frac{f' \cdot g - f \cdot g'}{g^2} $

\noindent
Ketten $ (f(g(x))'= f'(g(x)) \cdot g'(x) $
Potenzen $ (x^n)' = n x^{n-1} $

\noindent \textbf{Basics}

$(\ln(x))' =  \frac{1}{x}  \mid (\frac{1}{x})' =  \frac{-1}{x^2} \mid (\sin(x))' = \cos(x) \mid (\cos(x))' = -\sin(x)  $

\noindent 
\textbf{Shortcuts}
$(a^x)' = \ln(a) \cdot a^x \mid (\sqrt{x})' = \frac{1}{2 \sqrt{x}} \mid (\sqrt[n]{x})' = \frac{1}{n \cdot \sqrt[n]{x}^{n-1}} \mid (\frac{1}{x^2})' = \frac{-2}{x^3}$

\noindent
\textbf{Weitere}
$(\ln(f(x)))' = \frac{f'(x)}{f(x)} \mid \arctan(x)' = \frac{1}{x^2+1} \mid \arccos(x)' = \frac{-1}{\sqrt{1-x^2}}\mid \arcsin(x)' = \frac{1}{\sqrt{1-x^2}}$
\switchcolumn

\setcounter{section}{2}
\setcounter{subsection}{1}



\subsection{Richtungsableitungen}
Sei \( D \subseteq \mathbb{R}^n \) ein Gebiet und \( e \in \mathbb{R}^n \) eine Richtung mit \( \|e\| = 1 \).
Dann heißt eine Funktion \( f : D \to \mathbb{R} \) im Punkt \( a \in D \) \textbf{in Richtung \( e \) differenzierbar}, wenn der Grenzwert

$
\frac{\partial}{\partial e} f(a) := \partial_e f(a) = \lim_{t \to 0} \frac{f(a + t e) - f(a)}{t}
$
existiert.

\noindent
\textbf{Satz}
\noindent
Sei \( f : D \subseteq \mathbb{R}^n \to \mathbb{R} \) eine Funktion. Dann ist \( f \) in \( x_0 = (x_1^0, \ldots, x_n^0) \in D \) genau dann in Richtung \( e \) differenzierbar, wenn die Funktion
$
\varphi_e(t) := f(x_0 + t e)
$
im Punkt \( t_0 = 0 \) nach \( t \) differenzierbar ist. Weiter ist die Ableitung von \( f \) im Punkt \( x_0 \) in Richtung \( e \) gleich der Ableitung von \( \varphi_e(t) \) im Punkt \( t_0 = 0 \).

\subsection{Die Ableitung}
Sei $D \subseteq \mathbb R^n$ ein Gebiet, dann heißt $f:D \to \mathbb R^n$ differenzierbar in $a \in D$, wenn es $L:\mathbb R^n \to \mathbb R^m$ gibt mit  

$\lim \limits_{||h|| \to 0} \frac{||f(a+h) . f(a) -Lh||}{||h||} = 0$

\noindent
\textbf{Jacobimatrix \& Hessematrix}

$$f: \mathbb{R}^k \to \mathbb{R}^n;   J_f(a) = \begin{bmatrix} 
\frac{\partial f_1}{\partial x_1} &\dots &\frac{\partial f_1}{\partial x_k} \\ 
\vdots &\ddots &\vdots \\ 
\frac{\partial f_n}{\partial x_1} &\dots & \frac{\partial f_n}{\partial x_k} \end{bmatrix} \in \mathbb{R}^{n\times k}
$$
$$
H_f(a) = \begin{bmatrix}
\frac{\partial^2 f}{\partial^2 \partial x_1} & \frac{\partial^2 f}{\partial x_1 \partial x_2} &\dots &\frac{\partial^2 f}{ \partial x_1 \partial x_n}\\
\vdots & \vdots & \ddots & \vdots\\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial^2 x_2} &\dots &\frac{\partial^2 f}{\partial x_1 \partial x_n}\\
\frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_m \partial x_2} &\dots &\frac{\partial^2 f}{\partial  x_n \partial x_n}\\
\end{bmatrix}
$$

\noindent
\textbf{Tangentialebene an Punkt a $f: \mathbb R^n \to \mathbb R$}


$T(x) = f(a) + \nabla f(a) \cdot (x - a)$
mit $x,a \in \mathbb R^n$
beziehungsweise

$
T(x_1, \ldots, x_n) = f(a_1, \ldots, a_n)
+ \sum_{i=1}^n \frac{\partial f}{\partial x_i}(a_1, \ldots, a_n)\cdot (x_i - a_i)
$


\noindent
\textbf{Gradient $f: \mathbb R^n \to \mathbb R$}

$
\operatorname{grad}(f) = \nabla f = J_f^T = 
\begin{pmatrix}
    \frac{\partial f}{\partial x_1}\\
    \vdots \\
    \frac{\partial f}{\partial x_n}
\end{pmatrix}
$

\noindent
\textbf{Divergenz $f: \mathbb R^n \to \mathbb R^n$}

$\operatorname{div}(f) = \nabla f := \sum_{i=1}^n \frac{\partial f_i}{\partial x_i}$

\noindent
\textbf{Rotation: $f: \mathbb R^3 \to \mathbb R^3$}

$\nabla \times f(a) := \left( \frac{\partial f_3}{\partial x_2}(a) - \frac{\partial f_2}{\partial x_3}(a), \frac{\partial f_1}{\partial x_3}(a) - \frac{\partial f_3}{\partial x_1}(a), \frac{\partial f_2}{\partial x_1}(a) - \frac{\partial f_1}{\partial x_2}(a) \right)$

\noindent
\textbf{Lokale Auflösung / Satz von der impliziten Funktion}

\noindent
Sei $f: D \subseteq \mathbb{R}^2 \to \mathbb{R}$ stetig differenzierbar, $(a, b) \in D$ mit $f(a, b) = 0$ und $\frac{\partial f}{\partial y}(a, b) \neq 0$. Dann existiert eine offene Umgebung $I$ von $a$ und eine eindeutig bestimmte, stetig differenzierbare Funktion $h: I \to \mathbb{R}$ mit $h(a) = b$, sodass

$
f(x, h(x)) = 0 \quad \text{für alle } x \in I
$

\noindent
und es gibt ein $c > 0$, sodass für alle $(x, y) \in D$ mit $\|(x, y) - (a, b)\| < c$ und $f(x, y) = 0$ gilt: $y = h(x)$.

\noindent
\textbf{Taylorpolynome}

\noindent
$
T(a+h) = f(a) + \sum \limits_{i=1}^n\left( \frac{\partial f}{\partial x_i}(a) \cdot h_i \right)+   \sum \limits_{i=1}^n  \sum \limits_{j=1}^n \frac{1}{2} \left(\frac{\partial^2 f}{\partial x_i \partial x_j}(a) \cdot h_i h_j \right)+ R
$

\noindent
$
T(a+h) = f(a) +  J_f(a) h + \frac{1}{2} h^T H_f(a)h + R\cdot (x_1^2, \dots, x_n^2)
$

\end{paracol}

\begin{paracol}{3}



\newpage
\setcounter{section}{2}
\setcounter{subsection}{3}


\subsection{Analysis}
\textbf{Sattelpunkt, Extrema}:
$\mathbb{R} \to \mathbb{R} :f'(t) =0$ 

$\mathbb{R}^2 \to \mathbb{R}: \nabla f(t) = 0 \iff J_f(t) = 0$ (Nullmatrix)

\noindent
\textbf{Sattelpunkt in a} $\det(H_f(a)) < 0$ wobei gilt 

$\operatorname{sg}\left(\frac{\partial^2 f}{\partial^2 x}\right) \neq \operatorname{sg}\left(\frac{\partial^2 f}{\partial^2 y}\right)$

\noindent
\textbf{Extrema} 
$\det(H_f(a)) > 0$ und

Maximum: $H_f(a)$ negativ definit (z.B. $\frac{\partial^2 f}{\partial^2 x} < 0$)

Minimum: $H_f(a)$ positiv definit (z.B. $\frac{\partial^2 f}{\partial^2 x} > 0$)

\subsection{Lagrange-Funktion}
\textbf{Funktion} 
$f(x,y) = x^4 + y^5$

\noindent
\textbf{Nebenbedingung}
$30 = 2x + 2y \iff g(x,y) = 30 - 2x - 2y = 0$

\noindent
\textbf{Lagrange-Funktion}
$\mathcal L(x,y, \lambda) = f(x,y) - \lambda g(x,y)$

\noindent
\textbf{Vorgehen}
$\mathcal L(x,y,\lambda)$ jeweils nach $x,y,\lambda$ ableiten und daraus aufösen

\section{Mehrdimensionale Integrale}
\textbf{Unbestimmte Integrale in $\mathbb R$}
{\color{red} Bei Integralen + C nicht vergessen!}


\noindent
$
\int x\sin(x) dx = -x cos(x) + sin(x)
\mid
\int \cos(2x) dx = \frac{1}{2}sin(\frac{x}{2})
$

\noindent
$
\int \cos(x) dx = \sin(x) \mid \int \sin(x) dx = -\cos(x)
$

\noindent
$
\int \frac{1}{x} dx = ln(x)
\mid 
\int xe^{x^2} dx = \frac{e^{x^2}}{2}
\mid
\int \frac{1}{\sqrt{x}} = 2\sqrt{x}
$

\noindent
$
\int \frac{1}{\sqrt{a^2-x^2}} = \arcsin(\frac{x}{a}) \mid \int \frac{-1}{\sqrt{a^2-x^2}} = \arccos(\frac{x}{a})
\mid
\int \frac{1}{x^2+a^2} = \frac{1}{a}\arctan(\frac{x}{a})
$

\noindent \textbf{Partielle Integration}

$
\int f'(x) \cdot g(x) \mathrm{d}x = f(x)\cdot g(x) - \int f(x)\cdot g'(x)\,\mathrm{d}x\\
$
Wahl von f: $\ln(x) > \arcsin(x) > x^n > \sin(x) >e^x$

\section{Differenzialgleichungen}


\noindent
\textbf{Homogene DGs 1. Ordnung: Trennung der Variablen}

$$
\begin{aligned}
\dot{x} &= f(t)g(x)\\
\int \frac{1}{g(x)} dt &= \int f(t) dt
\end{aligned}
$$
$->$ Nach x auflösen 

\noindent
\textbf{Nichthomogene DGs 1. Ordnung}
Sei $\dot{x} = ax + g(t)$ Differenzialgleichung und $x_h(t)$ Lösung der zugehörigen homogenen Differentialgleichung $\dot{x} = ax$, dann ist $\color{red}\alpha x_h +x_p$ eine Lösung für die Diffentialgleichung. Beispiel Bestimmung von $x_p$:
$$
\dot{x} = 2x - 4t^2 -4t + 2
$$

Dann haben $x_p, \dot{x_p}$ die Form

$$
x_p = c_2t^2 + c_1t + c_0 \quad \texttt{und} \quad  \dot{x_p} = 2c_2t +c_1
$$

\noindent
Einsetzen
$$
\begin{aligned}
    \dot{x_p} - x_p &= (-4t^2 - 4t +2)\\
    (2c_2t + c_1) -2(c_2t^2 + c_1t + c_0) &= (-4t^2 - 4t +2)\\
\end{aligned}
$$
$\to$ Auflösen nach $c_1, c_2, c_3$

\switchcolumn



\newpage
\noindent
\textbf{Homogene DGs 2. Ordnung $\ddot{x} = ax + b\dot{x}$}

Ansatz : $x = e^{\lambda t}$

Charakteristisches Polynom: ${\color{red}( \lambda^2 - b\lambda -a)}e^{\lambda t}$

Nullstellen finden und Lambda einsetzen

\noindent
\textbf{Allgemeiner Ansatz für lineare DGLs mit konstanten koeffizienten}

Ausgangspunkt: $x^{(n)} = \sum_{i=0}^{n-1}a_i x^{(i)}$

Charakteristisches Polynom: $\lambda^n - \sum_{i=0}^{n-1} a_i \lambda^i$

$$
\begin{aligned}
\operatorname{P}(\lambda) &= \lambda^n - \sum_{i=0}^{n-1} a_i \lambda^i\\
&= \underbrace{\prod_{k=1}^{m_1} (\lambda - \lambda_k)^{s_k}}_{\text{Reelle Nullstellen}} + \underbrace{\prod_{k=m_1 + 1}^{m_1 + m_2} ((\lambda - \lambda_k)(\lambda - \overline{\lambda_k}))^{s_k}}_{\substack{\text{Komplexe Nullstellen} \\ \lambda_k = x_k+y_ki}}
\end{aligned}
$$

\noindent
Lösungsbasis:
%{\tiny
%$$
%\begin{matrix}
%e^{\lambda_1 t},te^{\lambda_1 t}, \dots , t^{s_1-1}e^{\lambda_1t}\\
%e^{\lambda_2 t},te^{\lambda_2 t}, \dots , t^{s_2-1}e^{\lambda_2t}\\
%\vdots \\
%e^{\lambda_{m_1} t},te^{\lambda_{m_1} t}, \dots , t^{s_{m_1}-1}e^{\lambda_{m_1}t}\\
%e^{x_{m_1 +1 }t} \cdot \cos(y_{m_1+1}t), e^{x_{m_1 +1 }t} \cdot \sin(y_{m_1+1}t)\\
%\vdots\\
%t^{s_{m_1+1}-1}e^{x_{m_1 +1 }t} \cdot \cos(y_{m_1+1}t), \quad\dots \sin(y_{m_1+1}t)\\
%\vdots\\
%e^{x_{m_1 +m_2}t} \cdot \cos(y_{m_1 +m_2}t), \quad\dots \sin(y_{m_1 +m_2}t)\\
%\vdots\\
%t^{s_{m_1 +m_2}-1}e^{x_{m_1 +m_2}t} \cdot \cos(y_{m_1 +m_2}t), \quad\dots \sin(y_{m_1 +m_2}t)\\
%\end{matrix}
%$$
%}

\noindent

Sei $\operatorname{grad}(k)$ Grad der Nullstelle k und $b>0$ o.b.d.a., dann gilt:
$$
\begin{aligned}
\{t^q e^{\lambda t} \mid P(\lambda) = 0 \land \lambda \in \mathbb{R} , t<\operatorname{grad}(\lambda) \} 
\\\cup \{t^q e^{a t} \sin(bt) \mid P(a + bi) = 0 \land t<\operatorname{grad}(a+bi) \} 
\\\cup \{t^q e^{a t} \cos(bt) \mid P(a + bi) = 0 \land t<\operatorname{grad}(a+bi) \}
\end{aligned}
$$
(Formell nicht ganz korrekt, aber hier anschaulicher)

%\noindent
%\textbf{Selbsttransponierende Tichymatrix}
%\noindent
%Eine Matrix \( T \in \mathbb{R}^{3 \times 1} \land T \in \mathbb{R}^{1 \times 3} \) heißt \emph{selbsttransponierende Tichymatrix}, wenn für das Transponieren gilt:
%\[
%T^T = T
%\]
%wobei \( T^T \) die Transponierte von \( T \) bezeichnet. Dies ist genau dann erfüllt, wenn
%\[
%T = 
%\begin{pmatrix}
%t_1 \\
%t_2 \\
%t_3
%\end{pmatrix}
%=
%\begin{pmatrix}
%t_1 & t_2 & t_3
%\end{pmatrix}
%\]
%und die Tichy-Bedingung
%\[
%t_1^2 = t_2^2 = t_3^2 = t_1 t_2 t_3 = -1 
%\]
%gilt. In diesem Fall ist die Tichymatrix ein Vektor, der unter Transposition seine Gestalt bewahrt und dabei stets freundlich bleibt.

\noindent
\textbf{Substitution}
Sei $\dot{x} = f(at+bx+c)$ Differentialgleichung. Substituiere $u = at+bx+c \implies \dot{u} = a+b \cdot \dot{x} = a+bf(u)$ 

Substitution $\to$ Lösen der DG $\to$ Rücksubstitution. Beispiel:
$$
\begin{aligned}
\dot{x} &= 2t-x, x(1) = 2\\
u(t) &= 2t-x\\
u'(t) &= 2 - u(t) = h(t)g(u)\\
h(t) &= 1, g(u) = 2-u\\
u(1) &= 2 \cdot 1 -2 + 0 = 0\\
\int \frac{1}{2-u} du &= \int 1 dt\\
-\ln(2-u) + a &= t \quad c_1 -c_2 = a\\
\ln(2-u) &= a-t\\
u &= 2-e^{a-t} \to a = \ln(2)\mid u(1) = 0\\
x &= 2t - u  \leftarrow u = 2t-x\\
x &= 2t - 2 + 2e^{-t}
\end{aligned}
$$

\switchcolumn
\newpage

\section{Anhang}
\noindent
\textbf{Matrixindizes}

$
\begin{bmatrix}
 a_{11} & a_{12} & \dots & a_{1_n}\\
 a_{21} & a_{22} & \dots & a_{2_n}\\
 \vdots & \vdots & \ddots & \vdots\\
 a_{m1} & a_{m2} & \dots & a_{mn}
\end{bmatrix} \in \mathbb{R}^{m\times n}
$

\noindent
\textbf{Inverse Matrix}

$
\begin{bmatrix}
 a & b \\
 c & d \\
\end{bmatrix}^{-1}
=
\frac{1}{ad-bc}
\begin{bmatrix}
 d & -b\\
 -c & a 
\end{bmatrix}
$

\noindent
\textbf{Determinante}

$
\det \left(
\begin{bmatrix} 
a & b & c \\ 
d & e & f \\ 
g & h & i 
\end{bmatrix} 
\right) = aei + bfg + cdh - ceg - bdi - afh
$

$
\det \left(
\begin{bmatrix} 
    a & b\\c & d 
\end{bmatrix}
\right)=ad-bc
$

\noindent
\textbf{Matrixmultiplikation}
\begin{figure}[h!]
  \includegraphics[width=15em]{Matrixmultiplikation.png}
\end{figure}

\noindent
$c_{ik} = \sum_{j=1}^n a_{ij} \cdot b_{jk}$

\noindent
\textbf{abc-Formel}
$f(x) = ax^2 + bx + c = 0$

$x_{1,2} = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}$

\noindent
\textbf{pq-formel}
$f(x) = x^2 + px + q$


$x_{1,2} = -\frac{p}{2} \pm \sqrt{\left( \frac{p}{2} \right)^2 - q}$


\noindent


\end{paracol}
\end{document}