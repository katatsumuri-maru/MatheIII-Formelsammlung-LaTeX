\documentclass[6pt,a4paper,landscape]{article}
\pagestyle{empty}
\usepackage[landscape, left=0.5cm, right=0.5cm, top=0.5cm, bottom=0.5cm]{geometry}
\usepackage{paracol}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\begin{document}
\footnotesize
\begin{paracol}{3}

\section{Funktionen in $\mathbb{R}^n$}
\subsection{Metrik}

Eine Funktion \( d: \mathbb{R}^n \times \mathbb{R}^n \to \mathbb{R}_0^+ \) mit \( \forall x, y, z \in \mathbb{R}^n \):

\noindent
\textbf{Positive Definitheit}

$
    d(x, y) \geq 0 \quad \text{und} \quad d(x, y) = 0 \iff x = y
$

\noindent
\textbf{Symmetrie}
$
    d(x, y) = d(y, x)
$

\noindent
\textbf{Dreiecksungleichung}
$
    d(x, y) \leq d(x, z) + d(z, y)
$

\noindent
\textbf{Triviale Metrik:}

$
d(x,y) = \begin{cases}
1 &\text{falls } x \neq y \\
0 &\text{falls } x = y \\
\end{cases}
$

\noindent
\textbf{Französische Eisenbahnmetrik:}

$
d(x,y) = \begin{cases}
||x-y|| &\text{falls } \exists \lambda \in \mathbb R : x = \lambda y \\
||x|| + ||y|| &\text{sonst} \\
\end{cases}
$

\subsection{Normen in $\mathbb R ^n$}

Eine Funktion $||.|| : \mathbb R^n \to \mathbb R^+_0$ mit

\noindent
\textbf{Positive Definitheit}
$
 ||x|| \ge 0 \land ||x|| = 0 \iff x = 0
$

\noindent
\textbf{Homogenität}
$
 ||\lambda x|| = |\lambda| ||x||
$

\noindent
\textbf{Dreiecksungleichung}
$
 ||x+y|| \le ||x|| + ||y||
$

\textbf{$l_1$-Norm}
$
||x||_1 = |x_1| + \dots + |x_n|
$

\textbf{$l_p$-Norm}
$
||x||_p = \sqrt[p]{x_1^p+\dots+x_n^p}
$

\textbf{Maximum-Norm}
$
||x||_\infty = \max\{ |x_i| : i=1, \dots, n\}
$

\noindent
\textbf{Satz}
Jede Norm induziert über $d(x,y) = ||x-y||$ eine Metrik.

\subsection{Bilinearform}
Eine Funktion $\langle.,. \rangle_A : \mathbb R^n \times \mathbb R^n \to \mathbb R$ heißt billinearform, wenn sie in beiden Argumenten linear ist:

$
\langle \alpha x + \beta y, z\rangle_A = \alpha \langle x,z \rangle_A + \beta \langle y,z \rangle_A
$

$
\langle z, \alpha x + \beta y \rangle_A = \alpha \langle z,x \rangle_A + \beta \langle z,y\rangle_A
$

\noindent
Weitere, mögliche Eigenschaften:

\textbf{Symmetrie}: $\forall x,y \in \mathbb R^n :\langle x,y \rangle_A = \langle y, x \rangle_A$

\textbf{Positive Definitheit}: $\forall x \in \mathbb R^n \land x \neq 0: \langle x,x \rangle_A > 0$

\noindent
Jede Quadratische Matrix $B$ erzeugt eine Bilinearform über $x^T B y$


\subsection{Topologie in $\mathbb R^n$}

\textbf{Offene Kugel mit Radius r um a:} 

$U_R(a) := \{x \in \mathbb R^n: ||x-a|| < r \}$

\noindent
\textbf{Innerer Punkt $a \in A \subseteq \mathbb R^n$:} 
$\exists \varepsilon > 0: U_\varepsilon(a) \subseteq A$

\noindent 
\textbf{Offenes Intervall:} Jeder Punkt ist innerer Punkt: 

$\forall a \in A : \exists \varepsilon > 0: U_\varepsilon(a) \subseteq A$
\indent bsp: $\mathbb R^n, \{\}, U_r(a)$

\noindent
\textbf{Abgeschlossenes Intervall:}
Komplement ist offen

bsp: $\mathbb R^n, \{ \}$

\noindent
\textbf{Beschränkt:}
$\exists M \in \mathbb R: A \subset U_M(0)$

\noindent
\textbf{Kompakt:} Abgeschlossen und beschränkt

\noindent
\textbf{Mengenoperationen}

$
(U \cup V)^C = U^C \cap V^C
$

$
(U \cap V)^C = U^C \cup V^C
$


\switchcolumn

\setcounter{section}{1}
\setcounter{subsection}{4}

\subsection{Folgen in $\mathbb{R}^n$}

\textbf{Konvergenz:}

$\exists g \in \mathbb R^n :\forall \varepsilon > 0: \exists M \in \mathbb N: \forall m \ge M: ||a_m -g|| < \varepsilon$

\noindent
\textbf{Mehrdimensionale Konvergenz:} Konvergiert genau dann, wenn alle Komponenten gegen den jeweiligen Wert konvergieren.

\noindent
\textbf{Cauchyfolge:}

$\forall \varepsilon > 0: \exists M \in \mathbb N; \forall s,m \ge M: ||a_s - a_m|| < \varepsilon$

\noindent
\textbf{Banachraum:}
Jede Cauchyfolge Konvergiert und die Metrik wird durch eine Norm induziert

Bsp: $R^n$ mit $l_p$-Norm

\subsection{Funktionen}
\textbf{Graph von $f: \mathbb R^n \to \mathbb R^m$:}

\noindent
$\Gamma_f := \{(x_1, \dots, x_n , z_1, \dots, z_m \in) D \times \mathbb R ^m: z_i = f_i(x_1, \dots, x_n)\}$

\noindent
\textbf{Höhen/Niveaulinie von $f: D \subseteq \mathbb R^2 \to \mathbb R$}

$H_c(f) := \{(x,y) \in D \mid f(x,y) = c\}$

\noindent \textbf{Niveaufläche von $f:D \subseteq \mathbb R^n \to \mathbb R$:}

$H_c(f) := \{(x_1,\dots, x_n) \in D \mid f(x_1, \dots, x_n) = c\}$

\noindent
\textbf{Eindimensionaler Schnitt}

$f(a_1, \dots, a_{i-1}, x_i, a_{i+1}, a_n)$ mit $a_k$ konst.

\noindent
\textbf{Kurve}

Ist $f : \mathbb R^n \to \mathbb R^m$ eine Funktion mit $f (x_1, \dots, x_n)$, dann ist $\phi : \mathbb R \to \mathbb R^n$ eine Kurve im $\mathbb R^n$ und $f (\phi(t))$ die Funktion entlang dieser Kurve.

\noindent
\textbf{Stetigkeit von Funktionen $\mathbb R^n \to \mathbb R^m$ ($\varepsilon-\delta$-Kriterium)}
Eine Funktion ist Stetig in einem Punkt a, gdw. Gilt:
$\forall \varepsilon > 0: \exists \delta >0: ||x-a|| < \delta \implies ||f(x) - f(a) || < \varepsilon$
\newline\noindent 
\textbf{Satz}
Eine Funktion $f : D \subseteq \mathbb R^n \to \mathbb R ^m$ ist genau dann stetig in $a \in D$, wenn dies
für die Funktionen $f_1,\dots, f_m$ gilt.

\noindent \textbf{Polynom in $n$ Variablen vom Grad $m$}

\noindent
\[
\sum_{i=0}^m \;\; \sum_{\substack{\alpha \in \mathbb{N}_0^n \\ |\alpha| = i}} a_\alpha \; x_1^{\alpha_1} \cdots x_n^{\alpha_n}
\]
mit $\alpha = (\alpha_1, \dots, \alpha_n)$ Multiindex, $a_\alpha \in \mathbb{R}$ (Vorfaktoren) und $|\alpha| = \alpha_1 + \dots + \alpha_n$.



\section{Differenzierbarkeit in $\mathbb R^n$}

\subsection{Ableitungsregeln in $\mathbb R^1$}

\noindent
\textbf{Linearität}
$ (f + g)' = f' + g' \qquad (c \cdot f)' = c \cdot f' $

\noindent
\textbf{Produktregel}
$ (f \cdot g)' = f' \cdot g + f \cdot g' $

\noindent
\textbf{Quotientenregel}
$ \left( f/g\right)' = \frac{f' \cdot g - f \cdot g'}{g^2} $

\noindent
\textbf{Kettenregel}
$ (f \circ g)'(x) = f'(g(x)) \cdot g'(x) $

\noindent
\textbf{Potenzregel}
$ (x^n)' = n x^{n-1} $

\noindent
\textbf{Ableitung des Logarithmus}
$ (\ln(f(x)))' = \frac{f'(x)}{f(x)} $

\noindent
\textbf{Ableitung der Sinusfunktion}
$ (\sin(f(x)))' = f'(x) \cdot \cos(f(x)) $

\noindent
\textbf{Ableitung der Kosinusfunktion}
$ (\cos(f(x)))' = -f'(x) \cdot \sin(f(x)) $

\noindent
\textbf{Weitere}

$(\frac{1}{x})' =  \frac{-1}{x^2}$

$(\ln(x))' =  \frac{1}{x}$

$\arctan(x)' = \frac{1}{x^2+1}$

$\arccos(x)' = \frac{-1}{\sqrt{1-x^2}}$

$\arcsin(x)' = \frac{1}{\sqrt{1-x^2}}$
\switchcolumn

\setcounter{section}{2}
\setcounter{subsection}{1}



\subsection{Richtungsableitungen}
Sei \( D \subseteq \mathbb{R}^n \) ein Gebiet und \( e \in \mathbb{R}^n \) eine Richtung mit \( \|e\| = 1 \).
Dann heißt eine Funktion \( f : D \to \mathbb{R} \) im Punkt \( a \in D \) \textbf{in Richtung \( e \) differenzierbar}, wenn der Grenzwert

$
\frac{\partial}{\partial e} f(a) := \partial_e f(a) = \lim_{t \to 0} \frac{f(a + t e) - f(a)}{t}
$
existiert.

\noindent
\textbf{Satz}
\noindent
Sei \( f : D \subseteq \mathbb{R}^n \to \mathbb{R} \) eine Funktion. Dann ist \( f \) in \( x_0 = (x_1^0, \ldots, x_n^0) \in D \) genau dann in Richtung \( e \) differenzierbar, wenn die Funktion
$
\varphi_e(t) := f(x_0 + t e)
$
im Punkt \( t_0 = 0 \) nach \( t \) differenzierbar ist. Weiter ist die Ableitung von \( f \) im Punkt \( x_0 \) in Richtung \( e \) gleich der Ableitung von \( \varphi_e(t) \) im Punkt \( t_0 = 0 \).

\subsection{Die Ableitung}
Sei $D \subseteq \mathbb R^n$ ein Gebiet, dann heißt $f:D \to \mathbb R^n$ differenzierbar in $a \in D$, wenn es $L:\mathbb R^n \to \mathbb R^m$ gibt mit  

$\lim \limits_{||h|| \to 0} \frac{||f(a+h) . f(a) -Lh||}{||h||} = 0$

\noindent
\textbf{Jacobimatrix \& Hessematrix}

$$J_f(a) = \begin{bmatrix} 
\frac{\partial f_1}{\partial x_1} &\dots &\frac{\partial f_1}{\partial x_k} \\ 
\vdots &\ddots &\vdots \\ 
\frac{\partial f_n}{\partial x_1} &\dots & \frac{\partial f_n}{\partial x_k} \end{bmatrix}
$$
$$
H_f(a) = \begin{bmatrix}
\frac{\partial^2 f}{\partial^2 \partial x_1} & \frac{\partial^2 f}{\partial x_1 \partial x_2} &\dots &\frac{\partial^2 f}{ \partial x_1 \partial x_n}\\
\vdots & \vdots & \ddots & \vdots\\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial^2 x_2} &\dots &\frac{\partial^2 f}{\partial x_1 \partial x_n}\\
\frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_m \partial x_2} &\dots &\frac{\partial^2 f}{\partial  x_n \partial x_n}\\
\end{bmatrix}
$$

\noindent
\textbf{Tangentialebene an Punkt a $f: \mathbb R^n \to \mathbb R$}


$T(x) = f(a) + \nabla f(a) \cdot (x - a)$
mit $x,a \in \mathbb R^n$
beziehungsweise

$
T(x_1, \ldots, x_n) = f(a_1, \ldots, a_n)
+ \sum_{i=1}^n \frac{\partial f}{\partial x_i}(a_1, \ldots, a_n)\cdot (x_i - a_i)
$


\noindent
\textbf{Gradient $f: \mathbb R^n \to \mathbb R$}

$
\operatorname{grad}(f) = \nabla f = J_f^T = 
\begin{pmatrix}
    \frac{\partial f}{\partial x_1}\\
    \vdots \\
    \frac{\partial f}{\partial x_n}
\end{pmatrix}
$

\noindent
\textbf{Divergenz $f: \mathbb R^n \to \mathbb R^n$}

$\operatorname{div}(f) = \nabla f := \sum_{i=1}^n \frac{\partial f_i}{\partial x_i}$

\noindent
\textbf{Rotation: $f: \mathbb R^3 \to \mathbb R^3$}

$\nabla \times f(a) := \left( \frac{\partial f_3}{\partial x_2}(a) - \frac{\partial f_2}{\partial x_3}(a), \frac{\partial f_1}{\partial x_3}(a) - \frac{\partial f_3}{\partial x_1}(a), \frac{\partial f_2}{\partial x_1}(a) - \frac{\partial f_1}{\partial x_2}(a) \right)$

\noindent
\textbf{Lokale Auflösung / Satz von der impliziten Funktion}

\noindent
Sei $f: D \subseteq \mathbb{R}^2 \to \mathbb{R}$ stetig differenzierbar, $(a, b) \in D$ mit $f(a, b) = 0$ und $\frac{\partial f}{\partial y}(a, b) \neq 0$. Dann existiert eine offene Umgebung $I$ von $a$ und eine eindeutig bestimmte, stetig differenzierbare Funktion $h: I \to \mathbb{R}$ mit $h(a) = b$, sodass

$
f(x, h(x)) = 0 \quad \text{für alle } x \in I
$

\noindent
und es gibt ein $c > 0$, sodass für alle $(x, y) \in D$ mit $\|(x, y) - (a, b)\| < c$ und $f(x, y) = 0$ gilt: $y = h(x)$.

\noindent
\textbf{Taylorpolynome}

\noindent
$
T(a+h) = f(a) + \sum \limits_{i=1}^n\left( \frac{\partial f}{\partial x_i}(a) \cdot h_i \right)+   \sum \limits_{i=1}^n  \sum \limits_{j=1}^n \frac{1}{2} \left(\frac{\partial^2 f}{\partial x_i \partial x_j}(a) \cdot h_i h_j \right)+ R
$

\noindent
$
T(a+h) = f(a) +  J_f(a) h + \frac{1}{2} h^T H_f(a)h + R\cdot (x_1^2, \dots, x_n^2)
$

\end{paracol}

\begin{paracol}{3}



\newpage
\setcounter{section}{2}
\setcounter{subsection}{3}


\subsection{Analysis}

\textbf{Sattelpunkt in a}

Erforderliche Eigenschaft:
$J_f(a) = 0$

Hinreichende Eigenschaft:
$\det(H_f(a)) < 0$

\noindent
\textbf{Maximum} 

Erforderliche Eigenschaft:
$J_f(a) = 0$

Hinreichende Eigenschaft:

$\det(H_f(a)) > 0$ und $H_f(a)$ negativ definit (z.B. $\frac{\partial^2 f}{\partial^2 x} < 0$)

\noindent
\textbf{Minimum}

Erforderliche Eigenschaft:
$J_f(a) = 0$

Hinreichende Eigenschaft:

$\det(H_f(a)) > 0$ und $H_f(a)$ positiv definit (z.B. $\frac{\partial^2 f}{\partial^2 x} > 0$)

\subsection{Lagrange-Funktion}
\textbf{Funktion} 
$f(x,y) = x^4 + y^5$

\noindent
\textbf{Nebenbedingung}
$30 = 2x + 2y \iff g(x,y) = 30 - 2x - 2y = 0$

\noindent
\textbf{Lagrange-Funktion}
$\mathcal L(x,y, \lambda) = f(x,y) - \lambda g(x,y)$

\noindent
\textbf{Vorgehen}
$\mathcal L(x,y,\lambda)$ jeweils nach $x,y,\lambda$ ableiten und daraus aufösen

\switchcolumn
\newpage


\noindent
\textbf{Selbsttransponierende Tichymatrix}

\noindent
Eine Matrix \( T \in \mathbb{R}^{3 \times 1} \land T \in \mathbb{R}^{1 \times 3} \) heißt \emph{selbsttransponierende Tichymatrix}, wenn für das Transponieren gilt:
\[
T^T = T
\]
wobei \( T^T \) die Transponierte von \( T \) bezeichnet. Dies ist genau dann erfüllt, wenn
\[
T = 
\begin{pmatrix}
t_1 \\
t_2 \\
t_3
\end{pmatrix}
=
\begin{pmatrix}
t_1 & t_2 & t_3
\end{pmatrix}
\]
und die Tichy-Bedingung
\[
t_1^2 = t_2^2 = t_3^2 = t_1 t_2 t_3 = -1 
\]
gilt. In diesem Fall ist die Tichymatrix ein Vektor, der unter Transposition seine Gestalt bewahrt und dabei stets freundlich bleibt.


\switchcolumn
\newpage

\section{Anhang}
\noindent
\textbf{Matrixindizes}

$
\begin{bmatrix}
 a_{11} & a_{12} & \dots & a_{1_n}\\
 a_{21} & a_{22} & \dots & a_{2_n}\\
 \vdots & \vdots & \ddots & \vdots\\
 a_{m1} & a_{m2} & \dots & a_{mn}
\end{bmatrix} \in \mathbb{R}^{m\times n}
$

\noindent
\textbf{Inverse Matrix}

$
\begin{bmatrix}
 a & b \\
 c & d \\
\end{bmatrix}^{-1}
=
\frac{1}{ad-bc}
\begin{bmatrix}
 d & -b\\
 -c & a 
\end{bmatrix}
$

\noindent
\textbf{Determinante}

$
\det \left(
\begin{bmatrix} 
a & b & c \\ 
d & e & f \\ 
g & h & i 
\end{bmatrix} 
\right) = aei + bfg + cdh - ceg - bdi - afh
$

$
\det \left(
\begin{bmatrix} 
    a & b\\c & d 
\end{bmatrix}
\right)=ad-bc
$

\noindent
\textbf{Matrixmultiplikation}
\begin{figure}[h!]
  \includegraphics[width=15em]{Matrixmultiplikation.png}
\end{figure}

\noindent
$c_{ik} = \sum_{j=1}^n a_{ij} \cdot b_{jk}$

\noindent
\textbf{abc-Formel}
$f(x) = ax^2 + bx + c = 0$

$x_{1,2} = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}$

\noindent
\textbf{pq-formel}
$f(x) = x^2 + px + q$


$x_{1,2} = -\frac{p}{2} \pm \sqrt{\left( \frac{p}{2} \right)^2 - q}$


\noindent
\textbf{Unbestimmte Integrale}

$
\int x\sin(x) dx = -x cos(x) + sin(x) + C
$

$
\int \cos(2x) dx = \frac{1}{2}sin(\frac{x}{2}) + C
$

$
\int \cos(x) dx = \sin(x) + C
$

$
\int \sin(x) dx = -\cos(x) + C
$

$\int \frac{1}{x} dx = ln(x) +C$

$\int xe^{x^2} dx = \frac{e^{x^2}}{2}$

$ 
\int_a^b f'(x)\cdot g(x) \mathrm{d}x = \left[f(x)\cdot g(x)\right]_{a}^{b} - \int_a^b f(x)\cdot g'(x)\,\mathrm{d}x\\$

\end{paracol}
\end{document}